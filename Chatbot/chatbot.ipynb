{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/emmanueluzodike/Natural_Language_Processing/blob/main/Chatbot/chatbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instructions on running chat bot\n",
        "\n",
        "To run the chatbot, you will need an apikey to be able to connect to OpenAI.\n",
        "\n",
        "Follow the below instructions to get your apikey:\n",
        "\n",
        "1. Sign up for an OpenAI account (if you already have an account, skip to step 3):\n",
        "Visit https://beta.openai.com/signup/ and sign up for an account using your email address and a strong password.\n",
        "\n",
        "2. Verify your email:\n",
        "After signing up, check your email inbox for a verification email from OpenAI. Click on the link provided in the email to verify your account.\n",
        "\n",
        "\n",
        "3. Log into your OpenAI account and Access your API key:\n",
        "Log in to your account at https://beta.openai.com/login/. Navigate to the \"API Keys\" tab or visit https://beta.openai.com/account/api-keys/. Here, you will find your API key, which should be a long string of letters and numbers.\n",
        "\n",
        "4. Keep your API key secure:\n",
        "Treat your API key like a password; do not share it with anyone, and avoid uploading it to public repositories or websites. If you suspect your API key has been compromised, you can regenerate a new key or revoke access from the API keys page."
      ],
      "metadata": {
        "id": "VK513QexvTXy"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6XHamYsBU8_h"
      },
      "source": [
        "# Library Imports"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "openai.api_key = \"<API-KEY>\" # Enter API key here"
      ],
      "metadata": {
        "id": "zlZanslKViq6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t7b-TDUlfFAr"
      },
      "outputs": [],
      "source": [
        "# Can comment out after single execution in same runtime\n",
        "\n",
        "## Download / install that take a long time\n",
        "import nltk\n",
        "#nltk.download('popular')\n",
        "#!pip install eng-spacysentiment # Uncomment this on first run\n",
        "\n",
        "## Python libraries\n",
        "from collections import defaultdict\n",
        "import string\n",
        "import pickle\n",
        "import random\n",
        "\n",
        "## Preprocessing\n",
        "from nltk import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from numpy import dot\n",
        "from numpy.linalg import norm\n",
        "import re\n",
        "\n",
        "# Prretrained NLP model\n",
        "#!pip install spacy # Uncomment this on first run\n",
        "#!python -m spacy download en_core_web_sm # Uncomment this on first run\n",
        "import spacy\n",
        "from spacy import displacy\n",
        "import eng_spacysentiment\n",
        "\n",
        "\n",
        "# Create objects to be \n",
        "wnl = WordNetLemmatizer()\n",
        "stopwords = stopwords.words('english')\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "sentiment = eng_spacysentiment.load()\n",
        "\n",
        "!pip install openpyxl"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Bxk80LgU2eh"
      },
      "source": [
        "# Prepping Knowledge Base"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import io\n",
        "import requests\n",
        "\n",
        "\n",
        "url = \"https://github.com/emmanueluzodike/Natural_Language_Processing/blob/main/Chatbot/data/ai_news.xlsx?raw=true\"\n",
        "\n",
        "response = requests.get(url)\n",
        "assert response.status_code == 200, f\"Failed to fetch file, status code: {response.status_code}\"\n",
        "\n",
        "content = io.BytesIO(response.content)\n",
        "df = pd.read_excel(content, engine='openpyxl')\n",
        "\n",
        "#df = df.set_index([\"Prompt\", \"completion\"])\n",
        "# Fill NaN values with empty string\n",
        "df.fillna('', inplace=True)\n",
        "\n",
        "df['content'] = df.Prompt +  \" .\" + df.completion\n",
        "df['completion'] = df['completion'].str.replace('\\n', ' ')\n",
        "df['content'] = df['content'].str.replace('\\n', ' ')\n",
        "df['Prompt'] = df['Prompt'].str.replace('\\n', ' ')\n",
        "df[\"tokens\"] = df[\"content\"].apply(lambda x: len(x.split()))\n",
        "#df = df.set_index([\"Prompt\", \"completion\"])\n",
        "print(f\"{len(df)} rows in the data.\")\n",
        "df.sample(5)"
      ],
      "metadata": {
        "id": "x3-f0Tuauv0F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lk5H3NIP24-H",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "outputId": "e4cf745c-909c-4215-b998-7fd5b67a0e14"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100 rows in the data.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                               Prompt  \\\n",
              "40  Bard gets a little smarter as Google adds some...   \n",
              "21  China's Alibaba invites businesses to trial AI...   \n",
              "83  Read the memo Google CFO Ruth Porat sent to st...   \n",
              "64  Alibaba's ChatGPT Rival to Offer Chinese and E...   \n",
              "8   It’s Way Too Easy to Get Google’s Bard Chatbot...   \n",
              "\n",
              "                                           completion  \\\n",
              "40  No offers foundGoogle's new AI should better u...   \n",
              "21  SHANGHAI, April 7 (Reuters) - Tech giant Aliba...   \n",
              "83  Jump to Google is continuing its effort to cut...   \n",
              "64  Your guide to a better future\"We are at a tech...   \n",
              "8   Vittoria ElliottWhen Google announced the laun...   \n",
              "\n",
              "                                              content  tokens  \n",
              "40  Bard gets a little smarter as Google adds some...     534  \n",
              "21  China's Alibaba invites businesses to trial AI...     473  \n",
              "83  Read the memo Google CFO Ruth Porat sent to st...     205  \n",
              "64  Alibaba's ChatGPT Rival to Offer Chinese and E...     348  \n",
              "8   It’s Way Too Easy to Get Google’s Bard Chatbot...     949  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-fad168b2-e3f2-487d-9bcf-541ad8d27423\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Prompt</th>\n",
              "      <th>completion</th>\n",
              "      <th>content</th>\n",
              "      <th>tokens</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>Bard gets a little smarter as Google adds some...</td>\n",
              "      <td>No offers foundGoogle's new AI should better u...</td>\n",
              "      <td>Bard gets a little smarter as Google adds some...</td>\n",
              "      <td>534</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>China's Alibaba invites businesses to trial AI...</td>\n",
              "      <td>SHANGHAI, April 7 (Reuters) - Tech giant Aliba...</td>\n",
              "      <td>China's Alibaba invites businesses to trial AI...</td>\n",
              "      <td>473</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>83</th>\n",
              "      <td>Read the memo Google CFO Ruth Porat sent to st...</td>\n",
              "      <td>Jump to Google is continuing its effort to cut...</td>\n",
              "      <td>Read the memo Google CFO Ruth Porat sent to st...</td>\n",
              "      <td>205</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>64</th>\n",
              "      <td>Alibaba's ChatGPT Rival to Offer Chinese and E...</td>\n",
              "      <td>Your guide to a better future\"We are at a tech...</td>\n",
              "      <td>Alibaba's ChatGPT Rival to Offer Chinese and E...</td>\n",
              "      <td>348</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>It’s Way Too Easy to Get Google’s Bard Chatbot...</td>\n",
              "      <td>Vittoria ElliottWhen Google announced the laun...</td>\n",
              "      <td>It’s Way Too Easy to Get Google’s Bard Chatbot...</td>\n",
              "      <td>949</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-fad168b2-e3f2-487d-9bcf-541ad8d27423')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-fad168b2-e3f2-487d-9bcf-541ad8d27423 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-fad168b2-e3f2-487d-9bcf-541ad8d27423');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 15
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: Runtime no longer has a reference to this dataframe, please re-run this cell and try again.\n",
            "Error: Runtime no longer has a reference to this dataframe, please re-run this cell and try again.\n",
            "Error: Runtime no longer has a reference to this dataframe, please re-run this cell and try again.\n"
          ]
        }
      ],
      "source": [
        "# df = pd.read_excel('/content/ai_news.xlsx') #<----------Enter file path to xlsx file\n",
        "# #df = df.set_index([\"Prompt\", \"completion\"])\n",
        "# # Fill NaN values with empty string\n",
        "# df.fillna('', inplace=True)\n",
        "\n",
        "# df['content'] = df.Prompt +  \" .\" + df.completion\n",
        "# df['completion'] = df['completion'].str.replace('\\n', ' ')\n",
        "# df['content'] = df['content'].str.replace('\\n', ' ')\n",
        "# df['Prompt'] = df['Prompt'].str.replace('\\n', ' ')\n",
        "# df[\"tokens\"] = df[\"content\"].apply(lambda x: len(x.split()))\n",
        "# #df = df.set_index([\"Prompt\", \"completion\"])\n",
        "# print(f\"{len(df)} rows in the data.\")\n",
        "# df.sample(5)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "2pVC0hxcedcT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lwZBoruE29SK"
      },
      "outputs": [],
      "source": [
        "df['content'] = df['content'].str.strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zOI5uAUX2_1n"
      },
      "outputs": [],
      "source": [
        "docs = []\n",
        "\n",
        "# iterate through each row of the DataFrame\n",
        "for index, row in df.iterrows():\n",
        "  # tokenize the `content` coulmn using NLTK's word_tokenize() function\n",
        "  tokens = nltk.word_tokenize(row['content'])\n",
        "  # append the list of tokens to the `docs` list\n",
        "  docs.append(tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tHMJ3UxP3C-t"
      },
      "outputs": [],
      "source": [
        "print(docs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eu08PHn53EKJ"
      },
      "outputs": [],
      "source": [
        "# preprocesses: remove non-alpha, remove stopwords, lemmatize\n",
        "docs_preprocessed = [[wnl.lemmatize(w) for w in doc if w not in stopwords and w.isalpha()] for doc in docs]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FH0bZEF13Gf4"
      },
      "outputs": [],
      "source": [
        "# create vocab set\n",
        "vocab = set()\n",
        "for doc in docs_preprocessed:\n",
        "  doc_set = set(doc)\n",
        "  vocab = vocab.union(doc_set)\n",
        "\n",
        "vocab = sorted(list(vocab))\n",
        "print('vocab length:', len(vocab))\n",
        "vocab[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TarEjSbq3J4b"
      },
      "outputs": [],
      "source": [
        "# creating vector\n",
        "vectors = []\n",
        "for doc in docs_preprocessed:\n",
        "    vec = [doc.count(t) for t in vocab]\n",
        "    vectors.append(vec)\n",
        "print(vectors[0][:10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sqWtpdsv3NBq"
      },
      "outputs": [],
      "source": [
        "# function to compute cosine similarity\n",
        "def cos_sim(v1, v2):\n",
        "    return float(dot(v1, v2)) / (norm(v1) * norm(v2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ylGBm-TB3R4G"
      },
      "outputs": [],
      "source": [
        "# compute cosine similarity for the first doc, paired with all docs\n",
        "for i, vec in enumerate(vectors):\n",
        "    print('cosine similarity anat1 and vector', i+1, '=', format(cos_sim(vectors[0], vec), '.2f'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V2BpWX2-3TH5"
      },
      "outputs": [],
      "source": [
        "print(docs_preprocessed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T3cvYTpS3Ues"
      },
      "outputs": [],
      "source": [
        "print(vectors[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v2xL4kiS3eHK"
      },
      "source": [
        "Find cosine similarity given a question"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LHgLmFSL3WSD"
      },
      "outputs": [],
      "source": [
        "# sample question\n",
        "question = \"How did Tesla's Model 3 perform in terms of sales?\"\n",
        "\n",
        "# preprocess the question\n",
        "question_tokens = nltk.word_tokenize(question)\n",
        "question_preprocessed = [wnl.lemmatize(w) for w in question_tokens if w not in stopwords and w.isalpha()]\n",
        "\n",
        "# create a vector for the question\n",
        "question_vector = [question_preprocessed.count(t) for t in vocab]\n",
        "\n",
        "# compute cosine similarity between the question vector and all document vectors\n",
        "for i, vec in enumerate(vectors):\n",
        "    print('cosine similarity between question and vector', i+1, '=', format(cos_sim(question_vector, vec), '.2f'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SJi29fI43kg_"
      },
      "source": [
        "Prompt contsruction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V5XD1JtZ3jTF"
      },
      "outputs": [],
      "source": [
        "def construct_prompt(question, vectors):\n",
        "  \"\"\"\n",
        "  Fetch relevant \n",
        "  \"\"\"\n",
        "  most_relevant_documents = get_relevant_documents(question, vectors)\n",
        "\n",
        "  # header = \"\"\"Answer the question as truthfully as possible using the provided context, and if the answer is not contained within the text below, say \"I don't know.\"\\n\\nContext:\\n\"\"\"\n",
        "  header = \"\"\"You are to take on the role of a chatbot. When a user asks a question, provide a response a conversational chat bot would and or Answer the question as truthfully as possible using the provided context, and if the answer is not contained within the text below, say \"I don't know.\"\\n\\nContext:\\n\"\"\"\n",
        "\n",
        "  return header + \" \" + str(most_relevant_documents[0]) + \" \" + \"\\n\\n Q: \" + question + \"\\n A:\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GjGAuwST3nc_"
      },
      "outputs": [],
      "source": [
        "def get_relevant_documents(query, vectors):\n",
        "  # compute cosine similarity for the query, paired with all docs\n",
        "  query_vec = [wnl.lemmatize(w) for w in word_tokenize(query) if w not in stopwords and w.isalpha()]\n",
        "  query_vec = [query_vec.count(t) for t in vocab]\n",
        "  similarities = [(i, cos_sim(query_vec, vec)) for i, vec in enumerate(vectors)]\n",
        "\n",
        "  # sort the similarities in descending order and select the top 3\n",
        "  top_3 = sorted(similarities, key=lambda x: x[1], reverse=True)[:3]\n",
        "  top_3_doc = []\n",
        "  # print the top 3 document contents and their similarity scores\n",
        "  for i, sim in top_3:\n",
        "      #print(f\"Document {i+1}: {df.loc[i, 'content']}\\nSimilarity score: {sim:.2f}\\n\")\n",
        "      content = df.loc[i, 'content'] \n",
        "      if len(content) > 4000:       # ensures prompt's token count does not exceed 4000\n",
        "          content = content[:4000]\n",
        "      top_3_doc.append(content)\n",
        "\n",
        "  return top_3_doc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZfxywyRk34yo"
      },
      "source": [
        "Give chatgpt prompt and context"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qynMrehx31Dq"
      },
      "outputs": [],
      "source": [
        "!pip install openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4--LruMV3_Gr"
      },
      "outputs": [],
      "source": [
        "import openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w2_v6fy44AEj"
      },
      "outputs": [],
      "source": [
        "# openai.api_key = \"<API-KEY>\"\n",
        "COMPLETIONS_MODEL = \"text-davinci-003\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mWb12tHM4BQ8"
      },
      "outputs": [],
      "source": [
        "COMPLETIONS_API_PARAMS = {\n",
        "    # We use temperature of 0.0 because it gives the most predictable, factual answer.\n",
        "    \"temperature\": 0.0,\n",
        "    \"max_tokens\": 300,\n",
        "    \"model\": COMPLETIONS_MODEL,\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cPsjiGd74DDr"
      },
      "outputs": [],
      "source": [
        "def answer_query_with_context(\n",
        "    query: str,\n",
        "    vectors,\n",
        "    show_prompt: bool = False\n",
        ") -> str:\n",
        "    prompt = construct_prompt(\n",
        "        query,\n",
        "        vectors,\n",
        "    )\n",
        "    \n",
        "    if show_prompt:\n",
        "        print(prompt)\n",
        "\n",
        "    response = openai.Completion.create(\n",
        "                prompt=prompt,\n",
        "                **COMPLETIONS_API_PARAMS\n",
        "            )\n",
        "\n",
        "    return response[\"choices\"][0][\"text\"].strip(\" \\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c_3a6H0eUPnP"
      },
      "source": [
        "# User Model Functionality"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ANA9FruvHz_N"
      },
      "source": [
        "### Approach 1: Rule-based methods combined with sentiment analysis and POS tagging"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JsZQf5YAcKS1"
      },
      "outputs": [],
      "source": [
        "def user_registration1(): \n",
        "  # Prompt user\n",
        "  new_user_introduction = input(\"Chatbot: What is your full name and date of birth (yyyymmdd)?\\nUser: \")\n",
        "  new_user_introduction = nlp(new_user_introduction)\n",
        "\n",
        "  # Parse user input\n",
        "  person_found = False\n",
        "  dob_found = False\n",
        "  for word in new_user_introduction.ents:\n",
        "    if word.label_ == \"PERSON\":\n",
        "      person_found = True\n",
        "      name = word.text\n",
        "    if word.label_ == \"DATE\":\n",
        "      dob_found = True\n",
        "      dob = word.text\n",
        "\n",
        "  # Data validation\n",
        "  while (person_found == False) or (not (' ' in name)):\n",
        "    name = input(\"Chatbot: Full name not detected. Please capitalize and space between.\\nUser: \")\n",
        "    annotated_name = nlp(name)\n",
        "    if annotated_name.ents[0].label_ == \"PERSON\":\n",
        "      person_found = True\n",
        "  while dob_found == False:\n",
        "    dob = input(\"Chatbot: Date not detected. Please use format mmddyyyyy (no delimiters, pad left 0)\\nUser: \")\n",
        "    annotated_dob = nlp(dob)\n",
        "    if annotated_dob.ents[0].label_ == \"DATE\":\n",
        "      dob_found = True\n",
        "\n",
        "  # Get more information\n",
        "  \"\"\"\n",
        "    Test input:  I am a computer science student. I hate having to study. I have 3 siblings named Vincent, Andrew, and Emily. I like watching movies and to play games. I am a big fan of the Cowboys.\n",
        "  \"\"\"\n",
        "  more_info = input(\"Chatbot: Tell me more about yourself!\\nUser: \")\n",
        "  sentences = more_info.split('.')\n",
        "  more_info = nlp(more_info)\n",
        "\n",
        "  # Analyze sentiment\n",
        "  sentiment_dict = defaultdict()\n",
        "  for sentence in sentences:\n",
        "    doc = sentiment(sentence)\n",
        "#    print(sentence, doc.cats)\n",
        "    attitude = '+' if doc.cats['negative'] < 0.4 else '-'\n",
        "    sentence = sentence.translate(str.maketrans('', '', string.punctuation))\n",
        "    for word in sentence.split(' '):\n",
        "      sentiment_dict[word] = attitude\n",
        "\n",
        "  pos_of_interest = {'VB', 'VBG', 'NNP', 'NNPS', 'NN', 'NNS'}\n",
        "  pos_dict = defaultdict(set)\n",
        "\n",
        "  # Add to dictionary \n",
        "  for word in more_info:\n",
        "    if word.tag_ in pos_of_interest:\n",
        "      pos_dict[word.tag_].add((word.text, sentiment_dict[word.text]))\n",
        "\n",
        "  # Save dictionary as user model \n",
        "  user_model_fname = name.replace(' ', '').lower() + dob + '.p'\n",
        "  pickle.dump(pos_dict, open(user_model_fname, 'wb'))\n",
        "\n",
        "  # Give user login\n",
        "  userID = user_model_fname[:-2]\n",
        "  convo_start = input(\"Chatbot: Nice to meet you! Your userID is \" + userID + \" for future conversations. What do you want to talk about today?\\nUser: \")\n",
        "\n",
        "  return convo_start\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D5rDAoNVi5o4"
      },
      "outputs": [],
      "source": [
        "def user_signIn1(): \n",
        "  userID = input(\"Chatbot: Welcome back! What is your userID?\\nUser: \")\n",
        "  user_model_fname = userID.lower() + \".p\"\n",
        "  try:\n",
        "    pos_dict = pickle.load(open(user_model_fname, 'rb'))\n",
        "  except:\n",
        "    option = input(\"Chatbot: Could not find that username. Enter 'Y' to try again, 'N' to register \")\n",
        "    while option != 'Y' and option != 'N':\n",
        "      option = input(\"Chatbot: Unexpected response. Please enter 'Y' or 'N'.\\nUser: \")\n",
        "    # User is new, create user model\n",
        "    if option == 'N':\n",
        "      convo_start = user_registration2() # Chang HERE for alt version\n",
        "      return convo_start\n",
        "    if option == 'Y':\n",
        "      convo_start = user_signIn1()\n",
        "  else: \n",
        "    if len(pos_dict) > 0:\n",
        "      pos_tuples = list(pos_dict.items())\n",
        "      pos = random.randint(0, len(pos_tuples)-1)\n",
        "      tag = pos_tuples[pos][0]\n",
        "      pos_list = list(pos_tuples[pos][1])\n",
        "      word = random.randint(0, len(pos_list)-1)\n",
        "      word = pos_list[word]\n",
        "\n",
        "      # Generate prompt based on POS and sentiment\n",
        "      word_text = word[0]\n",
        "      if tag == \"VB\" or tag == \"VBG\":\n",
        "        if tag == \"VB\": \n",
        "          if word[0][-1] == \"e\":\n",
        "            word_text = word[0][:-1] + \"ing\"\n",
        "          else:\n",
        "            word_text = word[0] + \"ing\"\n",
        "        if word[1] == \"+\":\n",
        "          prompt = \"Do you still enjoy \" + word_text + \"?\"\n",
        "        else:\n",
        "          prompt = \"Are you still stuck \" + word_text + \"?\"\n",
        "      elif tag == \"NNP\":\n",
        "        if word[1] == \"+\":\n",
        "          prompt = \"How is \" + word_text + \" doing? (Good as ever)?\"\n",
        "        else:\n",
        "          prompt = \"How is \" + word_text + \" doing? (Pesky as ever)?\"\n",
        "      elif tag == \"NNPS\":\n",
        "        if word[1] == \"+\":\n",
        "          prompt = \"How are the \" + word_text + \" doing? (Good as ever)?\"\n",
        "        else:\n",
        "          prompt = \"How are the \" + word_text + \" doing? (Pesky as ever)?\"\n",
        "      elif tag == \"NNS\":\n",
        "        if word[1] == \"+\":\n",
        "          prompt = \"Any good \" + word_text + \" lately?\"\n",
        "        else:\n",
        "          prompt = \"Any terrible \" + word_text + \" lately?\"\n",
        "      else:\n",
        "        if word[1] == \"+\":\n",
        "          prompt = \"Tell me about an exciting \" + word_text + \".\"\n",
        "        else:\n",
        "          prompt = \"Tell me about a bad \" + word_text + \".\"\n",
        "    else:\n",
        "      birthdate = ''.join(filter(str.isdigit, user_model_fname))\n",
        "      prompt = 'I don\\'t know much about you, but I know your birthdate is ' + birthdate + '. How is the weather today?'\n",
        "\n",
        "    greeting = input(\"Chatbot: \" + prompt + \"\\nUser: \")\n",
        "\n",
        "    # Respond to greeting and jump into second part of conversation\n",
        "    sentiment_greeting = sentiment(greeting)\n",
        "    if sentiment_greeting.cats['positive'] > 0.4:\n",
        "      convo_start = input(\"Chatbot: That's great! What would you like to talk about today?\\nUser: \")\n",
        "    else:\n",
        "      convo_start = input(\"Chatbot: That's tough.. What would you like to talk about today?\\nUser: \")\n",
        "\n",
        "    return convo_start "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Dw7kfAzIKzo"
      },
      "source": [
        "### Approach 2: Collect data more directly and let Chat-GPT generate elaborate reponse"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_lRAPgLY2doH"
      },
      "outputs": [],
      "source": [
        "def user_registration2(): \n",
        "  # Prompt user\n",
        "  new_user_introduction = input(\"Chatbot: What is your full name and date of birth (yyyymmdd)?\\nUser: \")\n",
        "  new_user_introduction = nlp(new_user_introduction)\n",
        "\n",
        "  # Parse user input\n",
        "  person_found = False\n",
        "  dob_found = False\n",
        "  for word in new_user_introduction.ents:\n",
        "    if word.label_ == \"PERSON\":\n",
        "      person_found = True\n",
        "      name = word.text\n",
        "    if word.label_ == \"DATE\":\n",
        "      dob_found = True\n",
        "      dob = word.text\n",
        "\n",
        "  # Data validation\n",
        "  while (person_found == False) or (not (' ' in name)):\n",
        "    name = input(\"Chatbot: Full name not detected. Please capitalize and space between.\\nUser: \")\n",
        "    annotated_name = nlp(name)\n",
        "    if annotated_name.ents[0].label_ == \"PERSON\":\n",
        "      person_found = True\n",
        "  while dob_found == False:\n",
        "    dob = input(\"Chatbot: Date not detected. Please use format mmddyyyyy (no delimiters, pad left 0)\\nUser: \")\n",
        "    annotated_dob = nlp(dob)\n",
        "    if annotated_dob.ents[0].label_ == \"DATE\":\n",
        "      dob_found = True\n",
        "\n",
        "  # Get more information\n",
        "  \"\"\"\n",
        "    Test input:  I am a computer science student. I hate having to study. I have 3 siblings named Vincent, Andrew, and Emily. I like watching movies and to play games. I am a big fan of the Cowboys.\n",
        "  \"\"\"\n",
        "  likes_set = set()\n",
        "  likes = input(\"Chatbot: What are your likes and interests?\\nUser: \")\n",
        "  likes = nlp(likes)\n",
        "  for i in likes.ents:\n",
        "    if i.label_ in {'EVENT', 'GPE', 'PERSON', 'MONEY', 'FAC', 'LANGUAGE', 'NORP', 'WORK_OF_ART', 'PRODUCT'}:\n",
        "      likes_set.add(i.text)\n",
        "  span = likes[:]\n",
        "  chunks = list(span.noun_chunks) # Make doc into iterable\n",
        "  for chunk in chunks: \n",
        "    if chunk.text != 'I':\n",
        "      likes_set.add(chunk.text)\n",
        "\n",
        "  dislikes_set = set()\n",
        "  dislikes = input(\"Chatbot: What are your dislikes?\\nUser: \")\n",
        "  dislikes = nlp(dislikes)\n",
        "  for i in dislikes.ents:\n",
        "    if i.label_ in {'EVENT', 'GPE', 'PERSON', 'MONEY', 'FAC', 'LANGUAGE', 'NORP', 'WORK_OF_ART', 'PRODUCT'}: \n",
        "      dislikes_set.add(i.text)\n",
        "\n",
        "  span = dislikes[:]\n",
        "  chunks = list(span.noun_chunks) # Make doc into iterable\n",
        "  for chunk in chunks:\n",
        "    if chunk.text != 'I':\n",
        "      dislikes_set.add(chunk.text)\n",
        "  \n",
        "\n",
        "  # Convert sets to strings\n",
        "  likes_str = ', '.join(likes_set)\n",
        "  dislikes_str = ', '.join(dislikes_set)\n",
        "  # Create dictionary\n",
        "  user_dict = {}\n",
        "  user_dict['Name: '] = name\n",
        "  user_dict['DOB (yyyymmdd): '] = dob\n",
        "  user_dict['Likes: '] =likes_str\n",
        "  user_dict['Dislikes: '] = dislikes_str\n",
        "\n",
        "  # Save dictionary as user model \n",
        "  user_model_fname = name.replace(' ', '').lower() + dob + '.p'\n",
        "  pickle.dump(user_dict, open(user_model_fname, 'wb'))\n",
        "\n",
        "  # Give user login\n",
        "  userID = user_model_fname[:-2]\n",
        "  convo_start = input(\"Chatbot: Nice to meet you! Your userID is \" + userID + \" for future conversations. What do you want to talk about today?\")\n",
        "\n",
        "  return convo_start"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E7txRD8bJFzO"
      },
      "outputs": [],
      "source": [
        "# TODO: Implement something similar to answer_query_with_context()\n",
        "# Param - string formatted like the following:\n",
        "'''\n",
        "    User: christinetrinh20001125\n",
        "    Name: Christine Trinh\n",
        "    DOB (yyyymmdd): 20001125\n",
        "    Likes: Spanish, movies, Lucy, a fan, the Cowboys\n",
        "    Dislikes: rude people, video games, Santa\n",
        "'''\n",
        "# Returns - string response from ChatGPT\n",
        "def temp(user_info):\n",
        "  header = \"\"\"I want you to take on the role of a chatbot. The below text represents the user information like Their name, dob, Likes and dislikes.\n",
        "              I will like to to generate a random question based on their likes or dislikes or any other question you may wish based on thier info. \"\\n\\nUser info:\\n\"\"\"\n",
        "\n",
        "  prompt = header + user_info\n",
        "\n",
        "  response = openai.Completion.create(\n",
        "            prompt=prompt,\n",
        "            **COMPLETIONS_API_PARAMS\n",
        "        )\n",
        "  \n",
        "\n",
        "\n",
        "  return response[\"choices\"][0][\"text\"].strip(\" \\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qqyOgMeWF4nx"
      },
      "outputs": [],
      "source": [
        "def user_signIn2():\n",
        "  userID = input(\"\\nChatbot: Welcome back! What is your userID?\\nUser: \")\n",
        "  user_model_fname = userID.lower() + \".p\"\n",
        "  try:\n",
        "    user_dict = pickle.load(open(user_model_fname, 'rb'))\n",
        "  except:\n",
        "    option = input(\"\\nChatbot: Could not find that username. Enter 'Y' to try again, 'N' to register \")\n",
        "    while option != 'Y' and option != 'N':\n",
        "      option = input(\"\\nChatbot: Unexpected response. Please enter 'Y' or 'N'.\\nUser: \")\n",
        "    # User is new, create user model\n",
        "    if option == 'N':\n",
        "      convo_start = user_registration2() # Chang HERE for alt version\n",
        "      return convo_start\n",
        "    if option == 'Y':\n",
        "      convo_start = user_signIn2()\n",
        "  else:\n",
        "        # Report \n",
        "    print('----Pickle File Contents----')\n",
        "    for k, v in user_dict.items():\n",
        "      print(k, v)\n",
        "    print('----------------------------')\n",
        "\n",
        "    # Build String to send to Chat-GPT\n",
        "    user_info = ''\n",
        "    for t, v in user_dict.items():\n",
        "      user_info = user_info + t + v + '\\n'\n",
        "\n",
        "    #print(user_info)\n",
        "    chatbot_greeting = temp(user_info)\n",
        "    user_greeting = input('\\nChatbot: Welcome back, ' + user_dict['Name: '] + '. ' + chatbot_greeting[10:] + '\\nUser: ')\n",
        "\n",
        "    # Respond to greeting and jump into second part of conversation\n",
        "    sentiment_greeting = sentiment(user_greeting)\n",
        "    if sentiment_greeting.cats['positive'] >= 0.4:\n",
        "      convo_start = input(\"\\nChatbot: That's great! What would you like to talk about today?\\nUser: \")\n",
        "    else:\n",
        "      convo_start = input(\"\\nChatbot: That's tough.. What would you like to talk about today?\\nUser: \")\n",
        "\n",
        "    return convo_start\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "smNhZEqaCPB-"
      },
      "outputs": [],
      "source": [
        "def begin_conversation():\n",
        "  # Get user logged in or registered\n",
        "  new_user = input(\"\\nChatbot: Hello, I am Chatbot. Have we spoken before? (Y/N)\\nUser: \")\n",
        "  while new_user != 'Y' and new_user != 'N':\n",
        "    new_user = input(\"\\nChatbot: Unexpected response. Please enter 'Y' or 'N'.\\nUser: \")\n",
        "  # User is new, create user model\n",
        "  if new_user == 'N':\n",
        "    convo_start = user_registration2() # Change HERE for alt approach\n",
        "  if new_user == 'Y':\n",
        "    convo_start = user_signIn2()\n",
        "\n",
        "  return convo_start\n",
        "      \n",
        "  \n",
        "        \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sdomvUfeiMsN"
      },
      "source": [
        "### Driver Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mVjbrKT34E0C",
        "outputId": "7c3896cc-0c69-4890-e7f7-c713cda8de9b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Chatbot: Hello, I am Chatbot. Have we spoken before? (Y/N)\n",
            "User: Y\n",
            "\n",
            "Chatbot: Welcome back! What is your userID?\n",
            "User: briancox2000041420000414\n",
            "----Pickle File Contents----\n",
            "Name:  Brian Cox 20000414\n",
            "DOB (yyyymmdd):  20000414\n",
            "Likes:  a fan, Lucy, Cowboys, the Cowboys, Spanish, movies\n",
            "Dislikes:  video games, Santa, rude people\n",
            "----------------------------\n",
            "\n",
            "Chatbot: Welcome back, Brian Cox 20000414. What is your favorite movie, Brian?\n",
            "User: I really like Spiderman 2. What about you?\n",
            "\n",
            "Chatbot: That's tough.. What would you like to talk about today?\n",
            "User: What are your thoughts about AI\n",
            "\n",
            "Chatbot: I think AI has the potential to be a powerful tool for good, but it is important to ensure that it is used responsibly and ethically. We need to ensure that AI systems are transparent and accountable, and that they are developed with safety in mind. We also need to ensure that AI is not used to discriminate against people or to cause harm.\n",
            "User: Are you aware of your existence?\n",
            "\n",
            "Chatbot: I'm aware that I'm a chatbot, but I'm not sure what my existence means. I don't know.\n",
            "User: okay good bye\n",
            "\n",
            "Chatbot: Goodbye! It was nice talking to you.\n",
            "User: q\n"
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "  convo_start = begin_conversation()\n",
        "# Note: Use a simple English name while registering\n",
        "  while convo_start != \"q\":\n",
        "    if convo_start == 'q':\n",
        "      print('Chatbot: Cya later!')\n",
        "      break\n",
        "    response = answer_query_with_context(convo_start, vectors)\n",
        "    convo_start = input('\\nChatbot: ' + response + '\\nUser: ')\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eKlBzazsC0GG"
      },
      "source": [
        "I like to watch movies. I am a fan of the Cowboys. I like to play with Lucy. I enjoy learning Spanish. \\\\\n",
        "I do not like video games. Nor do I like rude people, nor Santa. \\\\\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}